{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import joblib\n",
    "from data_prep.renderMPpose import *\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = joblib.load(\"../data/islrtc_hand_train/lhpts.pkl\") \n",
    "handpts = output[\"joints\"]\n",
    "dims = handpts[0].flatten().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, noise_dim, pose_dim):\n",
    "        \n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.gen = torch.nn.Sequential(\n",
    "            # Fully Connected Layer 1\n",
    "            torch.nn.Linear(\n",
    "                in_features=noise_dim,\n",
    "                out_features=240,\n",
    "                bias=True\n",
    "            ),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            # Fully Connected Layer 2\n",
    "            torch.nn.Linear(\n",
    "                in_features=240,\n",
    "                out_features=240,\n",
    "                bias=True\n",
    "            ),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            # Fully Connected Layer 3\n",
    "            torch.nn.Linear(\n",
    "                in_features=240,\n",
    "                out_features=240,\n",
    "                bias=True\n",
    "            ),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "            # Fully Connected Layer 4\n",
    "            torch.nn.Linear(\n",
    "                in_features=240,\n",
    "                out_features=pose_dim,\n",
    "                bias=True\n",
    "            ),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maxout(nn.Module):\n",
    "\n",
    "    def __init__(self, num_pieces):\n",
    "\n",
    "        super(Maxout, self).__init__()\n",
    "\n",
    "        self.num_pieces = num_pieces\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x.shape = (batch_size? x 625)\n",
    "\n",
    "        assert x.shape[1] % self.num_pieces == 0  # 625 % 5 = 0\n",
    "\n",
    "        ret = x.view(\n",
    "            *x.shape[:1],  # batch_size\n",
    "            x.shape[1] // self.num_pieces,  # piece-wise linear\n",
    "            self.num_pieces,  # num_pieces\n",
    "            *x.shape[2:]  # remaining dimensions if any\n",
    "        )\n",
    "        \n",
    "        # ret.shape = (batch_size? x 125 x 5)\n",
    "\n",
    "        # https://pytorch.org/docs/stable/torch.html#torch.max        \n",
    "        ret, _ = ret.max(dim=2)\n",
    "\n",
    "        # ret.shape = (batch_size? x 125)\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, pose_dim):\n",
    "        \n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.disc = torch.nn.Sequential(\n",
    "            # Fully Connected Layer 1\n",
    "            torch.nn.Linear(\n",
    "                in_features=pose_dim,\n",
    "                out_features=240,\n",
    "                bias=True\n",
    "            ),\n",
    "            torch.nn.ReLU(),\n",
    "            # Fully Connected Layer 2\n",
    "            torch.nn.Linear(\n",
    "                in_features=240,\n",
    "                out_features=240,\n",
    "                bias=True\n",
    "            ),\n",
    "            torch.nn.ReLU(),\n",
    "            # Fully Connected Layer 3\n",
    "            torch.nn.Linear(\n",
    "                in_features=240,\n",
    "                out_features=1,\n",
    "                bias=True\n",
    "            ),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 512\n",
    "num_steps = len(handpts)\n",
    "\n",
    "generator = Generator(dims, dims).to(device)\n",
    "discriminator = Discriminator(dims).to(device)\n",
    "\n",
    "discriminator_optimizer = optim.SGD(discriminator.parameters(), lr=0.0002, momentum=0.5)\n",
    "generator_optimizer = torch.optim.SGD(generator.parameters(), lr=0.0002, momentum=0.5)\n",
    "\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for i, real_pose in enumerate(handpts):\n",
    "        if i == num_steps:\n",
    "            break\n",
    "        real_pose_tensor = torch.tensor(real_pose/128, dtype=torch.float)\n",
    "        real_pose_tensor = real_pose_tensor.flatten().to(device)\n",
    "        \n",
    "        batch_size = real_pose_tensor.shape[0]\n",
    "        # Train Discriminator\n",
    "        for _ in range(8):\n",
    "        \n",
    "            fake_pose_tensor = generator(torch.randn(1, dims).to(device))\n",
    "            \n",
    "            real_outputs = discriminator(real_pose_tensor).view(-1)\n",
    "            lossD_real = criterion(real_outputs, torch.ones_like(real_outputs))\n",
    "            fake_outputs = discriminator(fake_pose_tensor).view(-1)\n",
    "            lossD_fake = criterion(fake_outputs, torch.zeros_like(fake_outputs))\n",
    "            \n",
    "            lossD = (lossD_real + lossD_fake) / 2\n",
    "            discriminator.zero_grad()\n",
    "            lossD.backward(retain_graph=True)\n",
    "\n",
    "            discriminator_optimizer.step()\n",
    "\n",
    "        # Train Generator\n",
    "        outputs = discriminator(fake_pose_tensor)\n",
    "        lossG = criterion(outputs, torch.ones_like(outputs))\n",
    "        generator.zero_grad()\n",
    "        lossG.backward()\n",
    "\n",
    "        generator_optimizer.step()\n",
    "        #print(epoch, i)\n",
    "    # Visualize Results\n",
    "    if epoch % 1 == 0:\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] , Loss D: {lossD:.4f}, loss G: {lossG:.4f}\")\n",
    "        with torch.no_grad():\n",
    "            fake = generator(torch.randn(1, dims).to(device)).reshape(-1, 1, 21, 2)\n",
    "            #data = real_pose_tensor.reshape(-1, 1, 21, 2)\n",
    "            fake_pts = fake[0][0].numpy()*128\n",
    "            #real_pts = data[0][0].numpy()*128\n",
    "            #real_img = np.zeros((128, 128, 3), dtype=np.uint8)\n",
    "            fake_img = np.zeros((128, 128, 3), dtype=np.uint8)\n",
    "            #display_single_hand_skleton(real_img, real_pts.astype(int))\n",
    "            display_single_hand_skleton(fake_img, fake_pts.astype(int))\n",
    "            #vis = np.concatenate((real_img, fake_img), axis=1)\n",
    "            cv2.imwrite(f'tmp/out_e{epoch}.png', fake_img)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(42, 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
